import math
import os
import time
from pathlib import Path

import numpy as np
import scipy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.modules.module import Module
from torch.nn.parameter import Parameter
from torch_geometric.utils import convert
from torch_scatter import scatter_add, scatter_max
from torch_sparse import spmm


class SpGraphAttentionLayer(nn.Module):
    """
    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903
    """

    def __init__(self, in_features, out_features, dropout, alpha, concat=True, layerN='', device="cpu"):
        super(SpGraphAttentionLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.alpha = alpha
        self.concat = concat
        self.layerN = layerN

        self.device = device

        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  # Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ñ€, ÑÐ°Ð¼Ñ‹Ð¹ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ð¹
        nn.init.xavier_normal_(self.W.data, gain=1.414)

        self.a = nn.Parameter(torch.zeros(size=(1, 2 * out_features)))  # x2 Ñ‚Ðº Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° h_i Ð¸ h_j ÑÐ½Ð°Ñ‡Ð°Ð»Ð° ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ
        nn.init.xavier_normal_(self.a.data, gain=1.414)

        self.bias = nn.Parameter(torch.zeros(size=(1, out_features)))
        nn.init.xavier_normal_(self.bias.data, gain=1.414)

        # attention param for path
        self.pathW = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        nn.init.xavier_normal_(self.pathW.data, gain=1.414)

        self.pathbias = nn.Parameter(torch.zeros(size=(1, out_features)))
        nn.init.xavier_normal_(self.pathbias.data, gain=1.414)

        self.patha = nn.Parameter(torch.zeros(size=(1, 2 * out_features)))
        nn.init.xavier_normal_(self.patha.data, gain=1.414)

        self.patha_2 = nn.Parameter(torch.zeros(size=(1, 2 * out_features)))
        nn.init.xavier_normal_(self.patha_2.data, gain=1.414)

        self.patha_3 = nn.Parameter(torch.zeros(size=(1, 2 * out_features)))
        nn.init.xavier_normal_(self.patha_3.data, gain=1.414)

        self.pathMerge = nn.Parameter(torch.zeros(size=(2 * out_features, out_features)))
        nn.init.xavier_normal_(self.pathMerge.data, gain=1.414)

        self.lenAtt = nn.Parameter(torch.zeros(size=(1, 2 * out_features)))
        nn.init.xavier_normal_(self.lenAtt.data, gain=1.414)

        self.dropout = nn.Dropout(dropout)
        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def gat_layer(self, input, adj, genPath=False, eluF=True):
        # input.shape == num_vertices (N) x hf (hidden_feature)
        N = input.size()[0]  # num of vertices
        edge = adj._indices()  # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð´Ð²ÑƒÑ…Ð¼ÐµÑ€Ð½Ñ‹Ð¹ Ñ‚ÐµÐ½Ð·Ð¾Ñ€, Ð³Ð´Ðµ
        # ÑƒÐºÐ°Ð·Ð°Ð½Ñ‹ Ñ€ÐµÐ±Ñ€Ð° Ð³Ñ€Ð°Ñ„Ð° (Ð² Ð²Ð¸Ð´Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²), Ð²ÐµÐ´ÑƒÑ‰Ð¸Ðµ Ð² Ð²ÐµÑ€ÑˆÐ¸Ð½Ñ‹
        # edge[0][s] == FROM
        # edge[1][s] == TO
        # adj[FROM][TO]  != 0
        h = torch.mm(input, self.W)  # N x projected (=8)
        h = h + self.bias  # h: N x out
        # Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð²ÐµÑ€ÑˆÐ¸Ð½Ñ‹ Ñ„Ð¸Ñ‡Ñƒ (Ð²ÐµÐºÑ‚Ð¾Ñ€) Ð¸Ð· projected (8) Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÐµÐ¹

        # Self-attention on the nodes - Shared attention mechanism
        # ÐšÐ°Ðº Ð¸ Ð² ÑÑ‚Ð°Ñ‚ÑŒÐµ. Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° (Ñ„Ð¸Ñ‡Ð¸)
        # ÐžÐ±Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ, Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð±ÐµÑ€ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ðµ Ð²ÐµÑ€ÑˆÐ¸Ð½Ñ‹, Ð¾Ñ‚ÐºÑƒÐ´Ð° ÐµÑÑ‚ÑŒ Ñ€ÐµÐ±Ñ€Ð¾
        # (edge[0][i], edge[1][i]) -> Ð¸Ð½Ð´ÐµÐºÑÑ‹ Ð²ÐµÑ€ÑˆÐ¸Ð½ Ð´Ð»Ñ i-Ð³Ð¾ Ñ€ÐµÐ±Ñ€Ð°
        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()  # edge_h: 2*D x (num_edges)
        # Ð—Ð°Ñ‚ÐµÐ¼ ÑƒÐ¼Ð½Ð¾Ð¶Ð°ÐµÐ¼ Ð¸Ñ… Ð½Ð° shared_attention Ð²ÐµÐºÑ‚Ð¾Ñ€
        edge_att = self.a.mm(edge_h).squeeze()  # Ð¡ÐºÐ°Ð»ÑÑ€Ð½Ð¾Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ. shape == [108365]
        # pos = 22
        # edge_att[pos] = alpha_ij Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÐµÑ€ÑˆÐ¸Ð½Ð°Ð¼Ð¸ adj[edge[0][pos]][edge[1][pos]]
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÑ Ð¿Ð¾ Ð´Ð¾Ñ€Ð¾Ð³Ðµ relu
        edge_e_a = self.leakyrelu(edge_att)  # edge_e_a: E   attetion score for each edge
        # pos = 22
        # edge_e_a[pos] -> Ð²ÐµÑ Ñ€ÐµÐ±Ñ€Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÐµÑ€ÑˆÐ¸Ð½Ð°Ð¼Ð¸
        # FROM = adj[edge[0][pos]]
        # TO = adj[edge[1][pos]]
        # adj[edge]
        if genPath:
            with torch.no_grad():
                edge_weight = edge_e_a
                # ÐžÐ±Ñ€Ð°Ñ‚Ð¸ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ `scatter_max`. Ð—Ð´ÐµÑÑŒ Ð²Ñ‚Ð¾Ñ€Ñ‹Ð¼ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð¼ Ð¸Ð´ÑƒÑ‚
                # Ð¸Ð½Ð´ÐµÐºÑÑ‹ Ð²ÑÐµÑ… Ð²ÐµÑ€ÑˆÐ¸Ð½, Ð¸Ð· ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¸ÑÑ…Ð¾Ð´ÑÑ‚ Ñ€ÐµÐ±Ñ€Ð° (Ð² Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚Ð°Ð½Ð¸Ñ).
                #
                # (1) outð‘–=max(outð‘–,maxð‘—(srcð‘—))
                # ÑÐ¼. https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/max.html
                # edge[0, :]
                # ÐŸÐ¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¼Ñ‹ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ ÑÑ€ÐµÐ´Ð¸ Ð²ÑÐµÑ… Ð²Ñ‹Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ñ€ÐµÐ±ÐµÑ€ Ð²ÐµÑ€ÑˆÐ¸Ð½Ñ‹.
                # Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÐ¼ Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼, ÑÐ¿Ñ€Ð¾ÐµÑ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° Ð¸Ð½Ð´ÐµÐºÑÑ‹ edge[0, :]
                #
                # Ð’ ÑÐ°Ð¼Ð¾Ð¼ ÐºÐ¾Ð½Ñ†Ðµ, ÐºÐ¾Ð³Ð´Ð°
                p_a_e = edge_weight - scatter_max(edge_weight, edge[0, :], dim=0, dim_size=N)[0][edge[0, :]]
                p_a_e = p_a_e.exp()
                p_a_e = p_a_e / (
                    scatter_add(p_a_e, edge[0, :], dim=0, dim_size=N)[edge[0, :]] + torch.Tensor([9e-15]).to(self.device)
                )

                scisp = convert.to_scipy_sparse_matrix(edge, p_a_e, N)
                # TODO: Ð’ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ ÑÐ°Ð¼Ð¾Ð³Ð¾ forward_pass - Ð¿ÐµÑ€ÐµÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÐµÑÐ° Ð²ÑÐµÐ³Ð¾ Ð³Ñ€Ð°Ñ„Ð° Ð² ÑÐ¶Ð°Ñ‚Ð¾Ð¼ Ð²Ð¸Ð´Ðµ.
                scipy.sparse.save_npz(str(Path(os.getcwd()) / "weights" / "att" / f'attmat_{self.layerN}'), scisp)

        edge_e = torch.exp(edge_e_a - torch.max(edge_e_a))  # edge_e: E
        e_rowsum = spmm(index=edge, value=edge_e, m=N, n=N, matrix=torch.ones(size=(N, 1)).to(self.device))  # e_rowsum: N x 1
        # Ð¡Ñ‚Ñ€Ð¾Ñ‡ÐºÐ° Ð²Ñ‹ÑˆÐµ Ð´ÐµÐ»Ð°ÐµÑ‚ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ Ð²ÐµÑ‰ÑŒ: Ð¾Ð½Ð° ÑƒÐ¼Ð½Ð¾Ð¶Ð°ÐµÑ‚ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ ÑÐ¼ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ Ð² ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ a_i_j - Ð²ÐµÑ Ñ€ÐµÐ±Ñ€Ð° Ð¼ÐµÐ¶Ð´Ñƒ v_i Ð¸ v_j
        # ÑƒÐ¼Ð½Ð¾Ð¶Ð°ÐµÑ‚ ÐµÐµ Ð½Ð° Ð²ÐµÐºÑ‚Ð¾Ñ€ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ Ð¸Ð· ÐµÐ´Ð¸Ð½Ð¸Ñ†. Ð§Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð¿Ð¾ Ð¸Ñ‚Ð¾Ð³Ñƒ? ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾, ÑÑƒÐ¼Ð¼Ð° Ð²ÐµÑÐ¾Ð², Ð¸ÑÑ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ð¸Ð· i-=Ð¾Ð¹ Ð²ÐµÑ€ÑˆÐ¸Ð½Ñ‹
        #
        edge_e = self.dropout(edge_e)  # add dropout improve from 82.4 to 83.8
        # edge_e: E
        # ÐÐ¸Ð¶Ðµ Ð¿Ñ€Ð¾Ð¸ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÑƒÐ¿ÐµÑ€ Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ Ð²ÐµÑ‰ÑŒ.
        # Ð£ Ð½Ð°Ñ ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð²ÐµÑÐ°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ÑÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð²ÐµÐºÑ‚Ð¾Ñ€-Ñ„Ð¸Ñ‡Ð¸ Ð²ÐµÑ€ÑˆÐ¸Ð½ (ÑÐºÐ°Ð»ÑÑ€Ð½Ð¾Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ `self.a` Ð¸ ÐºÐ¾Ð½ÐºÐ°Ñ‚Ð° Ð´Ð²ÑƒÑ… Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð²ÐµÑ€ÑˆÐ¸Ð½)
        # Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð½Ð°Ð´Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð²ÐµÑ€ÑˆÐ¸Ð½Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°.
        # (ÐºÐ°Ðº Ð² ÑƒÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¸  (3))
        # Ð¡ÑƒÐ¼Ð¼Ñƒ Ð²ÑÐµÑ… Ð²Ñ‹Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ð²ÐµÑÐ¾Ð², Ð²Ð¾Ð·Ð²ÐµÐ´ÐµÐ½Ð½Ñ‹Ñ… Ð² exp - Ð¼Ñ‹ ÑƒÐ¶Ðµ ÑÐ´ÐµÐ»Ð°Ð»Ð¸. ÐžÑÑ‚Ð°Ð»Ð¾ÑÑŒ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒ
        # ÐžÑÐ½Ð¾Ð²Ñ‹Ð²Ð°ÑÑÑŒ Ð½Ð° ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð²ÐµÑÐ°Ñ… Ñ€ÐµÐ±ÐµÑ€ - ÑƒÐ¼Ð½Ð¾Ð¶Ð°ÐµÐ¼ Ð¸Ñ…
        # Ð¡Ñ‚Ñ€Ð¾Ñ‡ÐºÐ° Ð½Ð¸Ð¶Ðµ Ð´ÐµÐ»Ð°ÐµÑ‚ ÑÑƒÐ¼Ð¼Ñƒ Ð²ÑÐµÑ… ÑÐ¾ÑÐµÐ´Ð½Ð¸Ñ… ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð², ÑƒÐ¼Ð½Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ñ… Ð½Ð° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ Ð²ÐµÑ
        h_prime = spmm(index=edge, value=edge_e, m=N, n=N, matrix=h)  # Ð’ÑÐµ ÐµÑ‰Ðµ N x (out_features)
        h_prime = h_prime.div(e_rowsum + torch.Tensor([9e-15]).to(self.device))  # h_prime: N x out
        print()
        if self.concat and eluF:
            return F.elu(h_prime)
        else:
            return h_prime

    def pathat_layer(self, input, pathM, pathlens, eluF=True):
        N = input.size()[0]
        pathh = torch.mm(input, self.pathW)
        pathh = pathh + self.pathbias  # h: N x out

        if not self.concat:  # if the last layer
            pathlens = [2]

        pathfeat_all = None
        for pathlen_iter in pathlens:
            i = pathM[pathlen_iter]['indices']
            v = pathM[pathlen_iter]['values']
            featlen = pathh.shape[1]
            pathlen = v.shape[1]
            pathfeat = tuple((pathh[v[:, i], :] for i in range(1, pathlen)))
            pathfeat = torch.cat(pathfeat, dim=1)
            pathfeat = pathfeat.view(-1, pathlen - 1, featlen)
            pathfeat, _ = torch.max(pathfeat, dim=1)  # seems max is better?
            # pathfeat = torch.mean(pathfeat, dim=1)     #
            att_feat = torch.cat((pathfeat, pathh[i[0, :], :]), dim=1).t()
            if pathlen_iter == 2:
                path_att = self.leakyrelu(self.patha_2.mm(att_feat).squeeze())
            else:
                path_att = self.leakyrelu(self.patha_3.mm(att_feat).squeeze())
            # softmax of p_a -> p_a_e
            path_att = path_att - scatter_max(path_att, i[0, :], dim=0, dim_size=N)[0][i[0, :]]
            path_att = path_att.exp()
            path_att = path_att / (scatter_add(path_att, i[0, :], dim=0, dim_size=N)[i[0, :]] + torch.Tensor([9e-15]).cuda())
            path_att = path_att.view(-1, 1)
            path_att = self.dropout(path_att)  # add dropout here of p_a_e
            w_pathfeat = torch.mul(pathfeat, path_att)
            h_path_prime = scatter_add(w_pathfeat, i[0, :], dim=0)
            # h_path_prime is the feature embedded from paths  N*feat

            if pathfeat_all is None:
                pathfeat_all = h_path_prime
            else:
                pathfeat_all = torch.cat((pathfeat_all, h_path_prime), dim=0)

        if len(pathlens) == 2:
            leni = torch.tensor(np.array(list(range(N)) + list(range(N)))).cuda()

            att_feat = torch.cat((pathfeat_all, pathh[leni, :]), dim=1).t()
            path_att = self.leakyrelu(self.lenAtt.mm(att_feat).squeeze())
            # softmax of p_a -> p_a_e
            path_att = path_att - scatter_max(path_att, leni, dim=0, dim_size=N)[0][leni]
            path_att = path_att.exp()
            path_att = path_att / (scatter_add(path_att, leni, dim=0, dim_size=N)[leni] + torch.Tensor([9e-15]).cuda())
            path_att = path_att.view(-1, 1)
            # path_att = self.dropout(path_att)         # add dropout here of p_a_e
            w_pathfeat = torch.mul(pathfeat_all, path_att)
            h_path_prime = scatter_add(w_pathfeat, leni, dim=0)

        if self.concat and eluF:
            return F.elu(h_path_prime)
        else:
            return h_path_prime

    def forward(self, input, adj, pathM, pathlens=[2], genPath=False, mode='GAT'):
        if not self.concat:  # if the last layer
            pathM = {}
            pathM[2] = {}
            pathM[2]['indices'] = adj._indices()
            pathM[2]['values'] = adj._indices().transpose(1, 0)

        return self.gat_layer(input, adj, genPath=genPath)


__all__ = ["SpGraphAttentionLayer"]
